#rmarkdown::render("BinaryModelSelection.rmd")
options(warn=-1)
options(repos='http://cran.rstudio.com/')
list.of.packages <- c('glmnet', 'yaml', 'randomForest', 'xgboost', 'lattice', 'shiny', 'gridExtra','lme4','pROC','ROCR','RODBC')
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,'Package'])]
if(length(new.packages))
install.packages(new.packages)
# Install pbkrtest, caret
if (!'pbkrtest' %in% installed.packages()[,'Package']){
pbkrtesturl <- 'https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-5.tar.gz'
install.packages(pbkrtesturl, repos=NULL, type='source')
library(pbkrtest)
}
list.of.packages <- c('caret')
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,'Package'])]
if(length(new.packages))
install.packages(new.packages)
# Load packages
library(yaml)
library(shiny)
library(lme4)
library(glmnet)
library(randomForest)
library(xgboost)
library(lattice)
library(gridExtra)
library(caret)
library(pROC)
library(ROCR)
library(RODBC)
yamlFile <- file.choose()
print (paste0("Yaml file loc: ", yamlFile));
config = yaml.load_file(yamlFile)
description = eval(parse(text=config$InputData[1]));
trainTestSplitFraction = eval(parse(text=config$InputData[2]));
if (loadRData == TRUE) {
RDatafileLoc = eval(parse(text=config$RDataSource[2]));
load(RDatafileLoc);
}
loadRData = eval(parse(text=config$RDataSource[1]));
if (loadRData == TRUE) {
RDatafileLoc = eval(parse(text=config$RDataSource[2]));
load(RDatafileLoc);
}
#rmarkdown::render("BinaryModelSelection.rmd")
options(warn=-1)
options(repos='http://cran.rstudio.com/')
list.of.packages <- c('glmnet', 'yaml', 'randomForest', 'xgboost', 'lattice', 'shiny', 'gridExtra','lme4','pROC','ROCR','RODBC')
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,'Package'])]
if(length(new.packages))
install.packages(new.packages)
# Install pbkrtest, caret
if (!'pbkrtest' %in% installed.packages()[,'Package']){
pbkrtesturl <- 'https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-5.tar.gz'
install.packages(pbkrtesturl, repos=NULL, type='source')
library(pbkrtest)
}
list.of.packages <- c('caret')
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,'Package'])]
if(length(new.packages))
install.packages(new.packages)
# Load packages
library(yaml)
library(shiny)
library(lme4)
library(glmnet)
library(randomForest)
library(xgboost)
library(lattice)
library(gridExtra)
library(caret)
library(pROC)
library(ROCR)
library(RODBC)
yamlFile <- file.choose()
#yamlFile = "C:\\Users\\remoteuser\\Source\\Repos\\DGADSGeneralRepo\\Code\\Utilities\\Modeling\\BinaryClassification\\BinaryClassification.yaml";
print (paste0("Yaml file loc: ", yamlFile));
# Get data and split train/test
config = yaml.load_file(yamlFile)
description = eval(parse(text=config$InputData[1]));
trainTestSplitFraction = eval(parse(text=config$InputData[2]));
## If RData is specified, load R data
loadRData = eval(parse(text=config$RDataSource[1]));
if (loadRData == TRUE) {
RDatafileLoc = eval(parse(text=config$RDataSource[2]));
load(RDatafileLoc);
}
## Alternatively, if SQL data source is specified, pull data from SQL source
loadSQLData = eval(parse(text=config$SQLSource[1]));
if (loadSQLData == TRUE) {
server = eval(parse(text=config$SQLSource[2]))
database = eval(parse(text=config$SQLSource[3]))
username = eval(parse(text=config$SQLSource[4]))
password = eval(parse(text=config$SQLSource[5]))
windriver = eval(parse(text=config$SQLSource[6]))
linuxdriver = eval(parse(text=config$SQLSource[7]))
query = eval(parse(text=config$SQLSource[8]))
driver <- ifelse(length(grep('linux',tolower(sessionInfo()$platform))) > 0,  linuxdriver, windriver);
connStr <- paste0('driver=',driver,';server=',server,';database=',database,';Uid=',username,';Pwd=',password, sep='');
dbhandle <- odbcDriverConnect(connStr)
trainDF <- sqlQuery(dbhandle, query)
odbcClose(dbhandle)
}
## SELECT RELVANT COLUMNS
targetCol <- config$targetCol[1];
featureColsTmp <- eval(parse(text=config$featureCols[1]));
if (is.null(featureColsTmp)) {featureCols <- setdiff(colnames(trainDF), targetCol)} else {featureCols = featureColsTmp}
featureExclude <- eval(parse(text=config$featureCols[2]));
if (!is.null(featureExclude)) {featureCols = setdiff(featureCols, featureExclude)}
trainDF <- trainDF [,c(targetCol,featureCols)]
## CONVERT SOME COLUMNS TO FACTORS
factorCols <- eval(parse(text=config$factorCols[1]));
if (!is.null(factorCols)) {for (i in 1:length(factorCols)) { trainDF[, factorCols[i]] <- make.names(as.factor(trainDF[, factorCols[i]])) }}
## Train test split
inTrain = createDataPartition(as.integer(rownames(trainDF)), p = trainTestSplitFraction)$Resample1;
trainData = trainDF[inTrain,];
testData = trainDF[-inTrain,];
print (paste0("Input data description: ", description));
print (paste0("Train/test split percent: ", trainTestSplitFraction));
head(trainData, 3);
View(testData)
View(trainData)
# Create the control object for cross validation and hyper-parameter sweeping
nFolds = config$nFolds[1]
nGridstoSweep = config$nGridstoSweep[1]
sweepStrategy = config$sweepStrategy[1]
modelSelectionFunction = config$modelSelectionFunction[1]
evaluationMetric = config$evaluationMetric[1]
controlObject <- trainControl (method = sweepStrategy, number = nFolds, selectionFunction = modelSelectionFunction, search = "grid", summaryFunction=twoClassSummary, classProbs = TRUE);
# Get glmnet parameters and create parameter grid
runGlmnet <- eval(parse(text=config$glmnetParams[1]));
if (runGlmnet == TRUE) {
glmnetParamsAlpha <- eval(parse(text=config$glmnetParams[2]));
glmnetParamsLambda <- eval(parse(text=config$glmnetParams[3]));
eGrid <- expand.grid(.alpha = glmnetParamsAlpha, .lambda = glmnetParamsLambda);
num_glmnetGrid <- min(nrow(eGrid), nGridstoSweep)
set.seed(123)
eGrid <- eGrid[sample(as.numeric(rownames(eGrid)), num_glmnetGrid),]
glmnetStandardize <- eval(parse(text=config$glmnetParams[4]));
glmnetFamily <- eval(parse(text=config$glmnetParams[5]));
}
# Get randomForest parameters and create parameter grid
runRf <- eval(parse(text=config$rfParams[1]));
if (runRf == TRUE)  {
mtryMultiplier <- eval(parse(text=config$rfParams[2]));
mtryCenter <- ceiling(sqrt(ncol(trainData)-1));
mtrySeqeunce <- unique(ceiling(mtryCenter*mtryMultiplier));
rfGrid <- expand.grid(.mtry = mtrySeqeunce);
num_rfGrid <- min(nrow(rfGrid), nGridstoSweep);
set.seed(123)
rfGrid <- data.frame(rfGrid[sample(as.numeric(rownames(rfGrid)), num_rfGrid),]); colnames(rfGrid) <- '.mtry';
rf_nTree = eval(parse(text=config$rfParams[3]));
rf_nodeSize = eval(parse(text=config$rfParams[4]));
}
# Get xgBoost parameters and create parameter grid
runXgBoost <- eval(parse(text=config$xgBoostParams[1]));
if (runXgBoost == TRUE) {
nrounds <- eval(parse(text=config$xgBoostParams[2]));
eta <- eval(parse(text=config$xgBoostParams[3]));
max_depth <- eval(parse(text=config$xgBoostParams[4]));
gamma <- eval(parse(text=config$xgBoostParams[5]));
colsample_bytree <- eval(parse(text=config$xgBoostParams[6]));
min_child_weight <- eval(parse(text=config$xgBoostParams[7]));
xgBoostObjective <- eval(parse(text=config$xgBoostParams[8]));
xgBoostGrid = expand.grid(.nrounds = nrounds, .eta = eta, .max_depth = max_depth, .gamma = gamma, .colsample_bytree = colsample_bytree, .min_child_weight = min_child_weight);
n_xgBoostgrid <- min(nrow(xgBoostGrid), nGridstoSweep);
set.seed(123)
xgBoostGrid <- xgBoostGrid[sample(as.numeric(rownames(xgBoostGrid)), n_xgBoostgrid),]
}
runXgBoost
targetCol <- config$targetCol[1];
featureColsTmp <- eval(parse(text=config$featureCols[1]));
if (is.null(featureColsTmp)) {featureCols <- setdiff(colnames(trainData), targetCol)} else {featureCols = featureColsTmp}
features <- featureCols[1];
for (f in 2:length(featureCols)) {features <- paste(features, '+', featureCols[f])}
trainFormula <- as.formula(paste(targetCol, "~", features));
print (trainFormula)
# Fit glmnet model
if (runGlmnet == TRUE) {
netFit <- train(trainFormula, data = trainData, family = glmnetFamily, method = "glmnet", standardize = glmnetStandardize, tuneGrid = eGrid,  trControl = controlObject, metric = evaluationMetric);
print (paste0("Train GlmNet Model: ", runGlmnet));
}
# Fit randomForest model
if (runRf == TRUE)  {
rfFit <- train (trainFormula, data = trainData, method = 'rf', ntree = rf_nTree, nodesize = rf_nodeSize, importance = TRUE, tuneGrid = rfGrid, trControl = controlObject, metric = evaluationMetric);
print (paste0("Train RandomForest Model: ", runRf));
}
# Fit xgBoost model
if (runXgBoost == TRUE) {
xgBTreeFit = train(trainFormula, data = trainData,  method = "xgbTree", trControl = controlObject, tuneGrid = xgBoostGrid, objective = xgBoostObjective, metric = evaluationMetric)
print (paste0("Train xgBoost Model: ", runXgBoost));
}
if (runGlmnet == TRUE &  runRf == TRUE & runXgBoost == TRUE) {allResamples <- resamples(list("glm-net" = netFit, "randomForest" = rfFit, "xgboost" = xgBTreeFit))} else if (runGlmnet == TRUE &  runRf == TRUE) {allResamples <- resamples(list("glm-net" = netFit, "randomForest" = rfFit))} else if (runGlmnet == TRUE &  runXgBoost == TRUE) {allResamples <- resamples(list("glm-net" = netFit, "xgboost" = xgBTreeFit))} else if (runRf == TRUE &  runXgBoost == TRUE) {allResamples <- resamples(list("randomForest" = rfFit, "xgboost" = xgBTreeFit))} else if (runGlmnet == TRUE) {allResamples <- resamples(list("glm-net" = netFit))} else if (runRf == TRUE) {allResamples <- resamples(list("randomForest" = rfFit))} else if (runXgBoost == TRUE) {allResamples <- resamples(list("xgboost" = xgBTreeFit))}
p1 <- bwplot(allResamples, metric='ROC', col='darkred', fill='lightblue', main = 'ROC vs. Algos', height=150, width=150)
p2 <- bwplot(allResamples, metric='Sens', col='darkred', fill='lightgreen', main = 'Sensitivity vs. Algos', height=150, width=150)
p3 <- bwplot(allResamples, metric='Spec', col='darkred', fill='gold', main = 'Specificty vs. Algos', height=150, width=150)
grid.arrange(p1,p2,p3, ncol=3)
# Get predictions from models
predictions <- data.frame(testData[,targetCol]); colnames(predictions) <- targetCol;
par(mfrow = c(1, 3))
# Create scatterplot for actual vs. glmnet predictions
if (runGlmnet == TRUE) {
predictions$glmnet <- predict(netFit, testData, type='prob')$X1;
pred <- prediction(predictions$glmnet, predictions[,targetCol]);
auc <- round(performance(pred,"auc")@y.values[[1]][1], 2); perf <- performance(pred,'tpr','fpr');
plot(perf, col='darkgreen', main = "Glmnet"); legend("bottomright", inset = 0.01, legend=paste0("AUC: ", auc), cex=1, box.lty=0)
}
# Create scatterplot for actual vs. randomForest predictions
if (runRf == TRUE) {
predictions$rf <- predict(rfFit, testData, type='prob')$X1;
pred <- prediction(predictions$rf, predictions[,targetCol]);
auc <- round(performance(pred,"auc")@y.values[[1]][1], 2); perf <- performance(pred,'tpr','fpr');
plot(perf, col='darkgreen', main = "Random Forest"); legend("bottomright", inset = 0.01, legend=paste0("AUC: ", auc), cex=1, box.lty=0)
}
# Create scatterplot for actual vs. xgboost predictions
if (runXgBoost == TRUE) {
predictions$xgb <- predict(xgBTreeFit, testData, type='prob')$X1;
pred <- prediction(predictions$xgb, predictions[,targetCol]);
auc <- round(performance(pred,"auc")@y.values[[1]][1], 2); perf <- performance(pred,'tpr','fpr');
plot(perf, col='darkgreen', main = "xgBoost"); legend("bottomright", inset = 0.01, legend=paste0("AUC: ", auc), cex=1, box.lty=0)
}
if (runGlmnet == TRUE) {vI <- varImp(object=netFit, useModel = TRUE, scale=TRUE);  p1 <- plot(vI, main = "glmNet", xlab='Relative Importance', ylab = 'Feature', top=10);}
if (runRf == TRUE) {vI <- varImp(object=rfFit, useModel = TRUE, scale=TRUE); p2 <- plot(vI, main = "randomForest", xlab='Relative Importance', ylab = 'Feature', top=10);}
if (runXgBoost == TRUE) {vI <- varImp(object=xgBTreeFit, useModel = TRUE, scale=TRUE); p3 <- plot(vI, main = "xgBoost", xlab='Relative Importance', ylab = 'Feature', top=10);}
if (runGlmnet == TRUE &  runRf == TRUE & runXgBoost == TRUE) {grid.arrange(p1,p2,p3, ncol=3)} else if (runGlmnet == TRUE &  runRf == TRUE) {grid.arrange(p1,p2, ncol=2)} else if (runGlmnet == TRUE &  runXgBoost == TRUE) {grid.arrange(p1,p3, ncol=2)} else if (runRf == TRUE &  runXgBoost == TRUE) {grid.arrange(p2,p3, ncol=2)} else if (runGlmnet == TRUE) {p1} else if (runRf == TRUE) {p2} else if (runXgBoost == TRUE) {p3}
